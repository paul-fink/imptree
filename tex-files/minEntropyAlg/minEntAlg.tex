\documentclass{beamer}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb} 
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{colortbl}


\theoremstyle{definition} \newtheorem{llemma}{Lemma}

\newcommand{\FORLOOP}[2]{\mathbf{for} \left(#1 \,\mathbf{to}\, #2\right)\, \mathbf{do} \,}
\newcommand{\WHILE}[1]{\mathbf{while} \left(#1 \right)\, \mathbf{do} \,}
\newcommand{\IFTHEN}[1]{\mathbf{if}\left(#1\right)\,\mathbf{then}\,\{ }
\newcommand{\ELSEIFTHEN}[1]{\}\,\mathbf{else}\,\mathbf{if}\left(#1\right)\,\mathbf{then}\,\{ }
\newcommand{\ELSE}{\}\,\mathbf{else}\,\{ }
\newcommand{\STATE}{\hspace*{0.5cm}}
\newcommand{\EXIT}{\mathbf{exit}}
\newcommand{\FOR}[1]{\mathbf{for} \left(#1\right)\, \mathbf{do} \,}

\newcommand{\Pb}{\mathrm{P}}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[text line]{%
  \parbox{\linewidth}{\vspace*{-8pt}Paul Fink, LMU Munich\hfill Minimum Entropy Algorithm\hfill\insertframenumber/\inserttotalframenumber}
}


\parindent0cm

\begin{document}
\title{Minimum Entropy Algorithm}
\author{Paul Fink, LMU Munich}

\begin{frame}
\begin{center}
\textbf{\Large{Minimum Entropy Algorithm}}\\[0.25em]
\textbf{\large{for NPI-generated F-probability intervals}}\\[1.5em]
\large{Paul Fink}\\
\begin{normalsize}
Department of Statistics, LMU Munich\\[1.5em]
17 December 2012\\
\end{normalsize}
\end{center}
\end{frame}

\begin{frame}{NPI 1}
Underlying model: Multinomial NPI with inference based upon the next observation\\[0.6em]
(Graphical) foundation on a probability wheel\\[0.6em]
$n$ observations create $n$ equidistant intervals on the circle, i.e slices on the wheel\\[0.6em]
Circular-$A_{(n)}$ assumption gives that next observation will fall into any given slice with probability $\frac{1}{n}$\\[0.6em]
Restrictions on the ordering of the observations onto the wheel.
\end{frame}

\begin{frame}{NPI 2}
Let us assume we have $n$ observations of $K$ different classes,\\
with $n_j \geq 0$ for $j = 1, \ldots, K$\\[0.6em]
Calculation of the class lower/upper probabilities based on one future observation:
\begin{displaymath}
\max(0, \frac{n_j-1}{n}) \leq \Pb(y_{n+1} = c_j) \leq \min(\frac{n_j+1}{n}, 1)
\end{displaymath}
$\Longrightarrow$ set of F-probability intervals
\end{frame}

\begin{frame}
Idea: Starting with the lower probabilities as working 'distribution',\\
then adding mass to classes until it is a probability distribution.\\[0.6em]
In what way to assign the mass?\\[0.8em]
Is it optimal?\\[1.4em]


Minimum entropy algorithm has already been developed for ordinal-NPI by \cite{minEntAlg}.
\end{frame}

\begin{frame}{Entropy}
Contribution of two classes to the complete entropy $H$:\\
\begin{displaymath}
H_1(x_1,x_2) := -\log(x_1)x_1 - \log(x_2)x_2.
\end{displaymath}
Entropy $H$ is concave function, so $H_1$ too.\\[0.6em]
Mass assignment of $m$ to either $x_1$ or $x_2$ or both.\\[0.6em]
Taking advantage of the concavity:
\begin{itemize}
\item $H_1(x_1 + m-c, x_2+c) \geq H_1(x_1+m, x_2) = H_1(x_1,x_2+m)$ for $0\leq c \leq m$ and $x_1 = x_2$\\
\item $H_1(x_1 + m, x_2) \leq H_1(x_1,x_2 + m)$ for $x_1 > x_2$\\
\item $H_1(x_1 + m, 0) \leq H_1(x_1,m)$ for $x_1 > 0$\\
\end{itemize}
\end{frame}

\begin{frame}{Algorithm outline}
Starting with lower probabilities, as these mass assignments are at least required.\\[0.6em]
In each step assigning as much remaining mass as possible to those classes with highest lower probability.\\[0.6em]
'as much mass as possible' is enforced by the corresponding upper probability or the probability distribution (sum to 1) 
\end{frame}

\begin{frame}{Minimum Entropy Algorithm for NPI}
\begin{scriptsize}
\begin{tabular}{ll}
Input: & Probability intervals ${[l_i, u_i]}^n_1$ as generated by the NPI\\
Output: & A probability distribution $\hat{p} = (\hat{p}_1, \hat{p}_2, \ldots , \hat{p}_n)$\\
\end{tabular}
\rule{\textwidth}{0.2mm}
Helping functions:\\
\begin{tabular}{ll}
Sum(x): & returns the sum of the elements of array x\\
getMaxIndex(x, S): & returns the first index of the maximum value\\
& of the array x considering only indices in S\\
\end{tabular}
\rule{\textwidth}{0.2mm}
Initialization:
S $\leftarrow$ ${1, \ldots, n}$\\[-0.8em]
\rule{\textwidth}{0.2mm}
$minEntropyNPI(l, u, \hat{p}) \{$\\
$\STATE\FORLOOP{i= 1}{n} \{\hat{p}_i \leftarrow l_i\}$\\
$\STATE mass \leftarrow 1 - Sum(\hat{p})$\\
$\STATE\WHILE{mass > 0} \{$\\
$\STATE\STATE index \leftarrow getMaxIndex(\hat{p}, S)$\\
$\STATE\STATE d \leftarrow u_{index} - \hat{p}_{index}$\\
$\STATE\STATE\IFTHEN{d \leq mass}$\\
$\STATE\STATE\STATE \hat{p}_{index} \leftarrow u_{index}$\\
$\STATE\STATE\STATE S \leftarrow S - \{index\}$\\
$\STATE\STATE\STATE mass \leftarrow mass - d$\\
$\STATE\STATE\ELSE$\\
$\STATE\STATE\STATE \hat{p}_{index} \leftarrow \hat{p}_{index} + mass$\\
$\STATE\STATE\STATE mass \leftarrow 0$\\
$\STATE\STATE\}$\\
$\STATE\}$\\
$\}$
\end{scriptsize}
\end{frame}

\begin{frame}{Properties}
Algorithm does not assign unobserved classes any mass.\\[1em]
Step-wise optimal\\[1em]
Minimum entropy distribution complies with the probability wheel\\[1em]
Algorithm gives only 1 minimum entropy distribution. There may be more!!\\[0.4em]
- negligible as main interest is entropy value, not underlying distribution\\
\end{frame}

\begin{frame}{Future Prospects}
Minimum and maximum entropy create entropy intervals as guarantee and potential\\[0.6em]
In case of classification trees:\\
Choosing on a split variable based on comparisons of those intervals\\
Reasonable opitmality criteria:
\begin{itemize}
\item Maximality (only taking the potential into account)
\item Interval dominance
\end{itemize}
\end{frame}

\begin{frame}{Reference}
\begin{thebibliography}{99}
\bibitem[Crossman, R.J. et al]{minEntAlg} Crossman, R.J., Abell\'{a}n, J., Augustin, T. and Coolen, F.P.A. (2011) Building Imprecise Classification Trees With Entropy Ranges.\emph{Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications}.\\
\end{thebibliography}
\end{frame}
\end{document}